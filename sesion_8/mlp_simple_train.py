# -*- coding: utf-8 -*-
"""MLP_simple_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GNdhMel5TBxAqKo8rxMdqg4PKnejQfrM

# Sistemas Embebidos
# Profesor: Cesar Hernando Valencia Niño

# Entrenamiento de una red MLP para implementación en Raspberry Pi

En este cuaderno vamos a entrenar una **red neuronal multicapa (MLP)** muy sencilla, cuyo objetivo es **clasificar el estado de un sistema** a partir de 4 entradas binarias. Estas 4 entradas representan el estado de un DIP switch de 4 posiciones que más adelante se conectará a una Raspberry Pi.

Cada combinación de los 4 bits de entrada se interpreta como una serie de condiciones del sistema (por ejemplo: temperatura alta, vibración, sobrecorriente, fallo previo). A partir de esa combinación, la red debe decidir entre tres posibles clases:

- **Clase 0 – NORMAL**  
- **Clase 1 – ADVERTENCIA**  
- **Clase 2 – CRÍTICO**

En lugar de usar datos reales, generamos un **conjunto de datos sintético**, donde:

1. Se construyen todas las combinaciones posibles de 4 bits (de 0000 a 1111).  
2. A cada combinación se le asigna una clase siguiendo una regla definida (por ejemplo, según la suma de bits).  
3. Se amplía el conjunto con más muestras añadiendo ligeras variaciones (ruido pequeño) para simular condiciones más realistas.

Con este conjunto de datos se entrena un **MLPClassifier de scikit-learn**, con:

- 4 neuronas de entrada (una por bit del DIP switch).  
- 1 capa oculta pequeña.  
- 3 neuronas de salida (una por cada clase).

Después del entrenamiento:

- Se evalúa el desempeño del modelo en un conjunto de prueba.  
- Se **extraen los pesos y sesgos** (W1, b1, W2, b2) de la red ya entrenada.  
- Estos pesos se copiarán en un script de Python que se ejecutará en la **Raspberry Pi**, donde se implementará la fase de **inferencia** (solo predicción, sin entrenamiento).

En la Raspberry Pi:

- El DIP switch proporcionará las 4 entradas binarias.  
- El modelo MLP embebido decidirá si el estado es NORMAL, ADVERTENCIA o CRÍTICO.  
- La decisión se mostrará mediante **LEDs** (verde, amarillo y rojo).
"""

import numpy as np
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# 1. Generar datos sintéticos
# Entradas: todas las combinaciones de 4 bits (0/1)
X_base = []
for a in [0,1]:
    for b in [0,1]:
        for c in [0,1]:
            for d in [0,1]:
                X_base.append([a,b,c,d])
X_base = np.array(X_base)

# Definimos una regla "industrial" para las clases (solo como ejemplo):
# - Clase 2 (CRÍTICO): si (a + b + c + d) >= 3
# - Clase 1 (ADVERTENCIA): si suma == 2
# - Clase 0 (NORMAL): si suma <= 1
y_base = []
for x in X_base:
    s = np.sum(x)
    if s >= 3:
        y_base.append(2)
    elif s == 2:
        y_base.append(1)
    else:
        y_base.append(0)
y_base = np.array(y_base)

# Para hacerlo más "ML", expandimos el dataset con ruido/repeticiones
rng = np.random.default_rng(42)
X_list, y_list = [], []
for _ in range(300):  # 300 ejemplos sintéticos
    idx = rng.integers(0, len(X_base))
    x = X_base[idx].astype(float)
    # Se puede agregar un pelín de ruido (si quieres), pero manteniendo [0,1]
    x_noisy = x + rng.normal(0, 0.05, size=4)
    X_list.append(x_noisy)
    y_list.append(y_base[idx])
X = np.vstack(X_list)
y = np.array(y_list)

print("Ejemplo X[0]:", X[0], "y[0]:", y[0])

# 2. Separar entrenamiento / prueba
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=0
)

# 3. Definir y entrenar el MLP
mlp = MLPClassifier(
    hidden_layer_sizes=(4,),   # 4 neuronas en la capa oculta
    activation='relu',
    solver='adam',
    max_iter=1000,
    random_state=0
)
mlp.fit(X_train, y_train)

# 4. Evaluación
y_pred = mlp.predict(X_test)
print(classification_report(y_test, y_pred))
print("Matriz de confusión:\n", confusion_matrix(y_test, y_pred))

# 5. Extraer pesos y sesgos
W1 = mlp.coefs_[0]    # (4 entradas x 4 neuronas ocultas)
W2 = mlp.coefs_[1]    # (4 ocultas x 3 salidas)
b1 = mlp.intercepts_[0]  # (4,)
b2 = mlp.intercepts_[1]  # (3,)

W1, b1, W2, b2

np.set_printoptions(precision=4, suppress=True)

print(f"W1 = np.array({W1.tolist()})")
print(f"b1 = np.array({b1.tolist()})")
print(f"W2 = np.array({W2.tolist()})")
print(f"b2 = np.array({b2.tolist()})")